PARTE CLONATA DAL REPOSITORY ORIGINALE


import torch
import torch.nn as nn


affine_par = True


IL MODELLO VIENE CREATO CON model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes) IN get_deeplab_v2 CHE VIENE POI CHIAMATO NEL TRAIN. (self, block, layers, num_classes, multi_level=False),
QUINDI GLI SI STA DICENDO CHE self._make_layer() ANDRA' A GENERARE I LAYER 1,2,3,4 CHE SONO DEI BOTTLENECK CON UN NUMERO DI FEATUREMAP UGUALE planes E UN NUMERO DI BLOCCHI DEFINITO IN  [3, 4, 23, 3]. Il Bottleneck viene “attaccato” alla ResNet nel momento in cui la ResNetMulti costruisce i layer layer1–layer4 usando _make_layer(...), e dipende dal parametro block che gli passi (in questo caso proprio Bottleneck).


A Bottleneck Residual Block is a residual block that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the ResNet architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101. COME DA INDICAZIONE DI PROGETTO, NEL NOSTRO CASO USIAMO R101, PER QUESTO VIENE IMPLEMENTATA LA CLASSE BOTTLENECK CHE E' COMPOSTA DA CONVOLUZIONI, BATCH NORM E RELU. NB. ANCHE IL BOTTLENECK è PRETRAINATO SU IMAGENET



class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)
        for p in self.bn1.parameters():
            p.requires_grad = False

        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,
                               padding=dilation, bias=False, dilation=dilation)
        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)
        for p in self.bn2.parameters():
            p.requires_grad = False

        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 4, affine=affine_par)
        for p in self.bn3.parameters():
            p.requires_grad = False

        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


È UN MODULO FINALE DI CLASSIFICAZIONE CHE PRENDE LE FEATURE ESTRATTE DALLA RESNET E PRODUCE UN OUTPUT PER OGNI CLASSSE, A LIVELLO DI PIXEL. NON E' 
PREADDESTRATO E VA A RAPPRESENTARE IL LAYER FINALE DEL RESNET

class ClassifierModule(nn.Module):
    def __init__(self, inplanes, dilation_series, padding_series, num_classes):
        super(ClassifierModule, self).__init__()
        self.conv2d_list = nn.ModuleList()
        for dilation, padding in zip(dilation_series, padding_series):
            self.conv2d_list.append(
                nn.Conv2d(inplanes, num_classes, kernel_size=3, stride=1,
                          padding=padding, dilation=dilation, bias=True)
            )

        for m in self.conv2d_list:
            nn.init.normal_(m.weight, mean=0, std=0.01)

    def forward(self, x):
        out = self.conv2d_list[0](x)
        for i in range(1, len(self.conv2d_list)):
            out += self.conv2d_list[i](x)
        return out



class ResNetMulti(nn.Module):
    def __init__(self, block, layers, num_classes, multi_level=False):
        super(ResNetMulti, self).__init__()
        self.inplanes = 64
        self.multi_level = multi_level

        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)
        for p in self.bn1.parameters():
            p.requires_grad = False

        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)

        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)

        self.layer6 = ClassifierModule(2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)   //CI ATTACCO IL CLASSIFIER FINALE

    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion, affine=affine_par)
            )
            for p in downsample[1].parameters():
                p.requires_grad = False

        layers = [block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample)]
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, dilation=dilation))

        return nn.Sequential(*layers)

    def forward(self, x):
        H, W = x.size(2), x.size(3)

        x = self.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.layer6(x)

        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True)

        if self.training:
            return x, None, None
        else:
            return x

LE PROSSIME FUNZIONI VANNO A INPOSTARE IL LEARNING RATE PER I DIVERSI LAYER, IMPOSTANDO
CHE IL LAYER FINALE AVRA' UN LEARNIGN RATE CHE è 10 VOLTE QUELLO BASE PERCHE' NON ESSENDO PRE-ADDESTRATO DEVE IMPARARE DA ZERO. I DUE GET SONO UNITI IN optim_parameters CHE VIENE POI CHIAMATO con optimizer = torch.optim.SGD(model.optim_parameters(lr), momentum=0.9, weight_decay=1e-4) NEL TRAIN


    def get_1x_lr_params_no_scale(self):
        for module in [self.conv1, self.bn1, self.layer1, self.layer2, self.layer3, self.layer4]:
            for m in module.modules():
                for p in m.parameters():
                    if p.requires_grad:
                        yield p

    def get_10x_lr_params(self):
        return self.layer6.parameters()

    def optim_parameters(self, lr):
        return [
            {'params': self.get_1x_lr_params_no_scale(), 'lr': lr},
            {'params': self.get_10x_lr_params(), 'lr': 10 * lr}
        ]

AGGIUNTA DA NOI:

import torch.nn.functional as F
import gdown
import os




def get_deeplab_v2(num_classes=19, pretrain=True):  
    model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes)   //ISTANZIA IL MODELLO


    BLOCCO DI CODICE PER SCARICARE I PESI DEL MODELLO PRETRAINATO
    pretrain_model_path = "deepLab_resenet_petrained_imagenet.pth"     
    file_id = "1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v"                      
    download_url = f"https://drive.google.com/uc?id={file_id}"         

    if pretrain and not os.path.exists(pretrain_model_path):
        print(">>> Downloading pretrained model...")          
        gdown.download(download_url, pretrain_model_path, quiet=False)    

    //MODIFICHE DELL'ORIGINALE
    if pretrain:
        print(">>> Loading pretrained weights...")
        saved_state_dict = torch.load(pretrain_model_path, map_location='cpu')  //Carica i pesi salvati da un modello pre-addestrat
        new_params = model.state_dict().copy()


PROF:
       for i in saved_state_dict:
            i_parts = i.split('.')
            new_params['.'.join(i_parts[1:])] = saved_state_dict[i]
NOI:

        for k, v in saved_state_dict.items():
            if k.startswith('module.'):
                k = k[7:]  # remove 'module.' if trained with DataParallel
            if k in new_params and new_params[k].size() == v.size():
                new_params[k] = v

        model.load_state_dict(new_params, strict=False) //Caricamento effettivo dei pesi

    return model


QUESTA FUNZIONE RITORNA IL MODELLO E DOVREBBE ESSERE USATA NEL TRAIN PER AVERE IL MODELLO PRETRAINATO:
resnet = get_deeplab_v2(num_classes=19, pretrain=True)

E NON 
resnet = ResNetMulti(Bottleneck, [3, 4, 6, 3], num_classes=19).cpu()

