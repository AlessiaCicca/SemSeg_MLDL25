# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Z_vU0UGKs8H2QDfKxDj9KDpVmledu-1
"""

import torch
import numpy as np
import time
from fvcore.nn import FlopCountAnalysis, flop_count_table

!pip install -U fvcore

#MEAN INTERSECTION OVER UNION: The Intersection over Union (IoU) metric, also referred to as the Jaccard index,
#is essentially a method to quantify the percent overlap between the target mask and our prediction output.

def calculate_iou(predicted_mask, target_mask):
    intersection = np.logical_and(predicted_mask, target_mask).sum()  #Calcola quanti pixel sono positivi sia nella predizione che nel target
    union = np.logical_or(predicted_mask, target_mask).sum()  #Calcola il numero totale di pixel che sono positivi in almeno una delle due maschere.
    iou = intersection / (union + 1e-10)  # Adding a small epsilon to avoid division by zero
    return iou


def evaluate_model(model, outputs, masks, input_size=(224, 224), iterations=1000, device='cpu'):
    print("\n=== MODEL EVALUATION ===")
    model.eval()
    model.to(device)

    # Assuming outputs and masks are tensors of shape [batch_size, num_classes, height, width]
    # Convert outputs and masks to numpy arrays
    outputs_np = outputs.cpu().detach().numpy()  # Detach from computational graph and move to CPU
    masks_np = masks.cpu().detach().numpy()

    iou_scores = []
    for i in range(len(outputs_np)):
        predicted_mask = np.argmax(outputs_np[i], axis=0)  # Assuming outputs are logits, take argmax
        target_mask = masks_np[i, 0]  # Assuming masks are binary or integer labels (0 for background)
        iou = calculate_iou(predicted_mask, target_mask)
        iou_scores.append(iou)


    # Calculate mean IoU and mean pixel accuracy over the entire validation set
    mean_iou = np.mean(iou_scores)
    print(f"Mean IoU: {mean_iou:.4f}")

    #LATECY: time to respond to a request or action

    # Simula un'immagine di input con 3 canali, altezza e larghezza personalizzabili
    height, width = input_size
    image = torch.rand(1, 3, height, width).to(device)  # batch size 1

    iterations = iterations
    latency = []
    FPS = []

    for _ in range(iterations):
        start = time.time()

        with torch.no_grad():  # evita il calcolo del gradiente
            _ = model(image)

        end = time.time()

        latency_i = end - start
        latency.append(latency_i)

        FPS_i = 1 / latency_i
        FPS.append(FPS_i)

    # Calcolo delle statistiche finali (in millisecondi per la latenza)
    meanLatency = np.mean(latency) * 1000
    stdLatency = np.std(latency) * 1000
    meanFPS = np.mean(FPS)
    stdFPS = np.std(FPS)

    print(f"Mean Latency: {meanLatency:.2f} ms")
    print(f"Std Latency: {stdLatency:.2f} ms")
    print(f"Mean FPS: {meanFPS:.2f}")
    print(f"Std FPS: {stdFPS:.2f}")

    #FLOPs:indicates the number of floating point operations performed in one second by the CPU.

    image_flop = torch.zeros((1, 3, height, width)).to(device)
    flops = FlopCountAnalysis(model, image_flop)
    print(flop_count_table(flops))

    #NUMBER OF PARAMETERS: misura: il numero totale di pesi all'interno del modello. Ad esempio, un layer fully connected con 100 input e 50 output
    #ha 100×50 = 5.000 parametri (più 50 bias se presenti). Più è alto, maggiore la capacità del modello, ma anche il rischio di overfitting e maggiore uso di memoria.

    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Trainable parameters: {trainable_params:,}")

#usa con evaluate_model(model, outputs, masks, input_size=(224, 224), iterations=1000, device='cuda') nel main MLDL_2025.ipynb
#impostando che il train come funzione e ritornando quei parametri sia per DeepLabV2 che per Realtime


#model	Il modello PyTorch che vuoi valutare. Serve per testare latenza, FPS, FLOPs e numero di parametri.
#outputs	L'output del modello (logits o probabilità) di dimensioni [batch_size, num_classes, H, W]. Serve per calcolare mIoU. #
#masks	Le maschere target (ground truth) dello stesso batch, tipicamente di forma [batch_size, 1, H, W].
#input_size	Una tupla (altezza, larghezza) per generare input fittizi (es. torch.rand(...)) quando si calcola latenza e FLOPs.
#iterations	Quante volte ripetere l'inferenza per stimare latenza e FPS medi. Valori più alti danno medie più stabili.
#device	"cpu" o "cuda" per scegliere se testare su CPU o GPU. Assicura che modello e tensori siano sullo stesso dispositivo.